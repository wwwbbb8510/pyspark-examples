{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pypark and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark is a good tool to load pyspark in notebook\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init spark session and define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to init a spark session\n",
    "def init_spark_session(app_name):\n",
    "    spark_session = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "    return spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init spark session\n",
    "app_name = 'pyspark example of basic functions'\n",
    "spark_session = init_spark_session(app_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base uri of hadoop file system\n",
    "hdfs_base_uri = 'hdfs://node-master:9000//user/hadoop/spark_examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user, transaction, user transaction data folder (csv and parquet)\n",
    "user_parq_folder = 'user_parq'\n",
    "user_csv_folder = 'user_csv'\n",
    "transaction_parq_folder = 'transaction_parq'\n",
    "transaction_csv_folder = 'transaction_csv'\n",
    "user_transaction_parq_folder = 'user_transaction_parq'\n",
    "user_transaction_csv_folder = 'user_transaction_csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fake table and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user table fields: 'id', 'email', 'first_name', 'last_name', 'created_date', 'modified_date'\n",
    "# generate 100 users\n",
    "user_schema = ['id', 'email', 'first_name', 'last_name', 'date_of_birth', 'created_date', 'modified_date']\n",
    "user_data = [(i, 'test_' + str(i) + '@gmail.com', 'fname_' + str(i%10+1), 'lname_' + str(i%10+1), datetime(1960+i%20, i%12+1, i%25+1),\n",
    "              datetime(2019, 8, i%30+1), datetime(2019, 9, i%30+1)) for i in range(1,101)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction table fields: 'id', 'amount', 'created_date'\n",
    "# generate 1000 transactions\n",
    "transaction_schema = ['id', 'amount', 'created_date']\n",
    "transaction_data = [(i, i%10+1, datetime(2019, 9, i%30+1)) for i in range(1,1001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user transaction relationship table fields: 'user_id', 'transaction_id'\n",
    "user_transaction_schema = ['user_id', 'transaction_id']\n",
    "user_transaction_data = [(i%100+1, i+1) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pyspark Dataframe from fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = spark_session.createDataFrame(user_data, user_schema)\n",
    "df_transaction = spark_session.createDataFrame(transaction_data, transaction_schema)\n",
    "df_user_transaction = spark_session.createDataFrame(user_transaction_data, user_transaction_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+---------+-------------------+-------------------+-------------------+\n",
      "| id|            email|first_name|last_name|      date_of_birth|       created_date|      modified_date|\n",
      "+---+-----------------+----------+---------+-------------------+-------------------+-------------------+\n",
      "|  1| test_1@gmail.com|   fname_2|  lname_2|1961-02-02 00:00:00|2019-08-02 00:00:00|2019-09-02 00:00:00|\n",
      "|  2| test_2@gmail.com|   fname_3|  lname_3|1962-03-03 00:00:00|2019-08-03 00:00:00|2019-09-03 00:00:00|\n",
      "|  3| test_3@gmail.com|   fname_4|  lname_4|1963-04-04 00:00:00|2019-08-04 00:00:00|2019-09-04 00:00:00|\n",
      "|  4| test_4@gmail.com|   fname_5|  lname_5|1964-05-05 00:00:00|2019-08-05 00:00:00|2019-09-05 00:00:00|\n",
      "|  5| test_5@gmail.com|   fname_6|  lname_6|1965-06-06 00:00:00|2019-08-06 00:00:00|2019-09-06 00:00:00|\n",
      "|  6| test_6@gmail.com|   fname_7|  lname_7|1966-07-07 00:00:00|2019-08-07 00:00:00|2019-09-07 00:00:00|\n",
      "|  7| test_7@gmail.com|   fname_8|  lname_8|1967-08-08 00:00:00|2019-08-08 00:00:00|2019-09-08 00:00:00|\n",
      "|  8| test_8@gmail.com|   fname_9|  lname_9|1968-09-09 00:00:00|2019-08-09 00:00:00|2019-09-09 00:00:00|\n",
      "|  9| test_9@gmail.com|  fname_10| lname_10|1969-10-10 00:00:00|2019-08-10 00:00:00|2019-09-10 00:00:00|\n",
      "| 10|test_10@gmail.com|   fname_1|  lname_1|1970-11-11 00:00:00|2019-08-11 00:00:00|2019-09-11 00:00:00|\n",
      "| 11|test_11@gmail.com|   fname_2|  lname_2|1971-12-12 00:00:00|2019-08-12 00:00:00|2019-09-12 00:00:00|\n",
      "| 12|test_12@gmail.com|   fname_3|  lname_3|1972-01-13 00:00:00|2019-08-13 00:00:00|2019-09-13 00:00:00|\n",
      "| 13|test_13@gmail.com|   fname_4|  lname_4|1973-02-14 00:00:00|2019-08-14 00:00:00|2019-09-14 00:00:00|\n",
      "| 14|test_14@gmail.com|   fname_5|  lname_5|1974-03-15 00:00:00|2019-08-15 00:00:00|2019-09-15 00:00:00|\n",
      "| 15|test_15@gmail.com|   fname_6|  lname_6|1975-04-16 00:00:00|2019-08-16 00:00:00|2019-09-16 00:00:00|\n",
      "| 16|test_16@gmail.com|   fname_7|  lname_7|1976-05-17 00:00:00|2019-08-17 00:00:00|2019-09-17 00:00:00|\n",
      "| 17|test_17@gmail.com|   fname_8|  lname_8|1977-06-18 00:00:00|2019-08-18 00:00:00|2019-09-18 00:00:00|\n",
      "| 18|test_18@gmail.com|   fname_9|  lname_9|1978-07-19 00:00:00|2019-08-19 00:00:00|2019-09-19 00:00:00|\n",
      "| 19|test_19@gmail.com|  fname_10| lname_10|1979-08-20 00:00:00|2019-08-20 00:00:00|2019-09-20 00:00:00|\n",
      "| 20|test_20@gmail.com|   fname_1|  lname_1|1960-09-21 00:00:00|2019-08-21 00:00:00|2019-09-21 00:00:00|\n",
      "+---+-----------------+----------+---------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show user dataframe\n",
    "df_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------------+\n",
      "| id|amount|       created_date|\n",
      "+---+------+-------------------+\n",
      "|  1|     2|2019-09-02 00:00:00|\n",
      "|  2|     3|2019-09-03 00:00:00|\n",
      "|  3|     4|2019-09-04 00:00:00|\n",
      "|  4|     5|2019-09-05 00:00:00|\n",
      "|  5|     6|2019-09-06 00:00:00|\n",
      "|  6|     7|2019-09-07 00:00:00|\n",
      "|  7|     8|2019-09-08 00:00:00|\n",
      "|  8|     9|2019-09-09 00:00:00|\n",
      "|  9|    10|2019-09-10 00:00:00|\n",
      "| 10|     1|2019-09-11 00:00:00|\n",
      "| 11|     2|2019-09-12 00:00:00|\n",
      "| 12|     3|2019-09-13 00:00:00|\n",
      "| 13|     4|2019-09-14 00:00:00|\n",
      "| 14|     5|2019-09-15 00:00:00|\n",
      "| 15|     6|2019-09-16 00:00:00|\n",
      "| 16|     7|2019-09-17 00:00:00|\n",
      "| 17|     8|2019-09-18 00:00:00|\n",
      "| 18|     9|2019-09-19 00:00:00|\n",
      "| 19|    10|2019-09-20 00:00:00|\n",
      "| 20|     1|2019-09-21 00:00:00|\n",
      "+---+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show transaction dataframe\n",
    "df_transaction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|user_id|transaction_id|\n",
      "+-------+--------------+\n",
      "|      1|             1|\n",
      "|      2|             2|\n",
      "|      3|             3|\n",
      "|      4|             4|\n",
      "|      5|             5|\n",
      "|      6|             6|\n",
      "|      7|             7|\n",
      "|      8|             8|\n",
      "|      9|             9|\n",
      "|     10|            10|\n",
      "|     11|            11|\n",
      "|     12|            12|\n",
      "|     13|            13|\n",
      "|     14|            14|\n",
      "|     15|            15|\n",
      "|     16|            16|\n",
      "|     17|            17|\n",
      "|     18|            18|\n",
      "|     19|            19|\n",
      "|     20|            20|\n",
      "+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show user transaction dataframe\n",
    "df_user_transaction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save pyspark dataframe as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function of saving pyspark dataframe\n",
    "def save_pyspark_dataframe(df_data, folder, format='csv'):\n",
    "    saved_folder = hdfs_base_uri + '/' + folder\n",
    "    if format == 'csv':\n",
    "        df_data.write.csv(saved_folder, header=True)\n",
    "    elif format == 'parquet':\n",
    "        df_data.write.parquet(saved_folder)\n",
    "    return saved_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/user_parq\\nhdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/user_csv\\nhdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/transaction_parq\\nhdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/transaction_csv\\nhdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/user_transaction_parq\\nhdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/user_transaction_csv\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# command to delete parquet and csv files \n",
    "'''\n",
    "hdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/user_parq\n",
    "hdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/user_csv\n",
    "hdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/transaction_parq\n",
    "hdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/transaction_csv\n",
    "hdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/user_transaction_parq\n",
    "hdfs dfs -rm -r hdfs://node-master:9000/user/hadoop/spark_examples/user_transaction_csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://node-master:9000//user/hadoop/spark_examples/user_transaction_parq'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save user, transaction and user transaction dataframes as parquet files\n",
    "save_pyspark_dataframe(df_user, user_parq_folder, 'parquet')\n",
    "save_pyspark_dataframe(df_transaction, transaction_parq_folder, 'parquet')\n",
    "save_pyspark_dataframe(df_user_transaction, user_transaction_parq_folder, 'parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://node-master:9000//user/hadoop/spark_examples/user_transaction_csv'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save user, transaction and user transaction dataframes as csv files\n",
    "save_pyspark_dataframe(df_user, user_csv_folder, 'csv')\n",
    "save_pyspark_dataframe(df_transaction, transaction_csv_folder, 'csv')\n",
    "save_pyspark_dataframe(df_user_transaction, user_transaction_csv_folder, 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pyspark dataframes from parquet or csv files and register as temporary views/tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a parquet folder as a table/view\n",
    "def load_table_from_parquet_file(table_name, parquet_path):\n",
    "    loaded = False\n",
    "    try:\n",
    "        lc_parquet_file = spark_session.read.option(\"mergeSchema\", \"true\").parquet(parquet_path)\n",
    "        lc_parquet_file.createOrReplaceTempView(table_name)\n",
    "        loaded = True\n",
    "    except:\n",
    "        pass\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a csv folder as a table/view\n",
    "def load_table_from_csv_file(table_name, csv_path):\n",
    "    loaded = False\n",
    "    try:\n",
    "        lc_csv_file = spark_session.read.option(\"mergeSchema\", \"true\").csv(csv_path, header=True)\n",
    "        lc_csv_file.createOrReplaceTempView(table_name)\n",
    "        loaded = True\n",
    "    except:\n",
    "        pass\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load user data from parquet file with the table name of user\n",
    "load_table_from_parquet_file('user', hdfs_base_uri + '/' + user_parq_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load transaction data from csv file with the table name of transaction\n",
    "load_table_from_csv_file('transaction', hdfs_base_uri + '/' + transaction_csv_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load user transaction data from parquet file with the table name of user_transaction\n",
    "load_table_from_parquet_file('user_transaction', hdfs_base_uri + '/' + user_transaction_parq_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pyspak sql queries to gather and manipulate data\n",
    "**join user, transaction by using user_transaction table and find the latest transaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to register pyspark dataframe as a table/view\n",
    "def register_dataframe_as_table(df_data, table_name):\n",
    "    loaded = False\n",
    "    try:\n",
    "        df_data.createOrReplaceTempView(table_name)\n",
    "        loaded = True\n",
    "    except:\n",
    "        pass\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select u.id u_id,t.id t_id, t.created_date from user_transaction ut  left join user u on ut.user_id=u.id left join transaction t on ut.transaction_id=t.id  order by t.created_date desc '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql to join user and transaction through user_transaction and sort user transaction by created_date of transaction\n",
    "# so the latest transacton will be on top\n",
    "sql = \"select u.id u_id,t.id t_id, t.created_date\" \\\n",
    "        \" from user_transaction ut \" \\\n",
    "        \" left join user u on ut.user_id=u.id\" \\\n",
    "        \" left join transaction t on ut.transaction_id=t.id \" \\\n",
    "        \" order by t.created_date desc \" \n",
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_transaction_sorted = spark_session.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------------+\n",
      "|u_id|t_id|        created_date|\n",
      "+----+----+--------------------+\n",
      "|  79| 179|2019-09-30T00:00:...|\n",
      "|  49| 149|2019-09-30T00:00:...|\n",
      "|  69| 569|2019-09-30T00:00:...|\n",
      "|  99| 599|2019-09-30T00:00:...|\n",
      "|  29|  29|2019-09-30T00:00:...|\n",
      "|  29| 629|2019-09-30T00:00:...|\n",
      "|  59| 659|2019-09-30T00:00:...|\n",
      "|  89| 689|2019-09-30T00:00:...|\n",
      "|  19| 719|2019-09-30T00:00:...|\n",
      "|  39| 539|2019-09-30T00:00:...|\n",
      "|   9| 809|2019-09-30T00:00:...|\n",
      "|  39| 839|2019-09-30T00:00:...|\n",
      "|  69| 869|2019-09-30T00:00:...|\n",
      "|  99| 899|2019-09-30T00:00:...|\n",
      "|  29| 929|2019-09-30T00:00:...|\n",
      "|  59| 959|2019-09-30T00:00:...|\n",
      "|  89| 989|2019-09-30T00:00:...|\n",
      "|  49| 749|2019-09-30T00:00:...|\n",
      "|   9| 509|2019-09-30T00:00:...|\n",
      "|  79| 779|2019-09-30T00:00:...|\n",
      "+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show sorted user transaction\n",
    "df_user_transaction_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register the dataframe of the sorted user transaction as a table: user_transaction_sorted\n",
    "register_dataframe_as_table(df_user_transaction_sorted, 'user_transaction_sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select u_id, first(t_id) latest_t_id, first(created_date) created_date  from user_transaction_sorted uts  group by u_id'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql to find the latest transation id\n",
    "sql = \"select u_id, first(t_id) latest_t_id, first(created_date) created_date \" \\\n",
    "        \" from user_transaction_sorted uts \" \\\n",
    "        \" group by u_id\"\n",
    "sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_latest_transaction = spark_session.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------------------+\n",
      "|u_id|latest_t_id|        created_date|\n",
      "+----+-----------+--------------------+\n",
      "|  26|        626|2019-09-27T00:00:...|\n",
      "|  29|        629|2019-09-30T00:00:...|\n",
      "|  65|        265|2019-09-26T00:00:...|\n",
      "|  19|        719|2019-09-30T00:00:...|\n",
      "|  54|        654|2019-09-25T00:00:...|\n",
      "|  22|        622|2019-09-23T00:00:...|\n",
      "|   7|        507|2019-09-28T00:00:...|\n",
      "|  77|        777|2019-09-28T00:00:...|\n",
      "|  34|        534|2019-09-25T00:00:...|\n",
      "|  50|        650|2019-09-21T00:00:...|\n",
      "|  94|        594|2019-09-25T00:00:...|\n",
      "|  57|        657|2019-09-28T00:00:...|\n",
      "|  32|         32|2019-09-03T00:00:...|\n",
      "|  43|        143|2019-09-24T00:00:...|\n",
      "|  84|        684|2019-09-25T00:00:...|\n",
      "|  31|        531|2019-09-22T00:00:...|\n",
      "|  39|        539|2019-09-30T00:00:...|\n",
      "|  98|        798|2019-09-19T00:00:...|\n",
      "|  25|         25|2019-09-26T00:00:...|\n",
      "|  95|        795|2019-09-16T00:00:...|\n",
      "+----+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show user latest transaction\n",
    "df_user_latest_transaction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use python function to calculate field for each row\n",
    "**cacluate user age based on user date of birth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the age\n",
    "curr_datetime = datetime.now()\n",
    "def calculate_age_from_dob(row):\n",
    "    age = None\n",
    "    dob_datetime = row.date_of_birth\n",
    "    try:\n",
    "        curr_month_datetime = datetime(2018, curr_datetime.month, curr_datetime.day)\n",
    "        dob_month_datetime = datetime(2018, dob_datetime.month, dob_datetime.day)\n",
    "\n",
    "        age = curr_datetime.year - dob_datetime.year \n",
    "        age = age if curr_month_datetime >= dob_month_datetime else age - 1\n",
    "    except:\n",
    "        print('out of range' + str(dob_str))\n",
    "    return Row(id=row.id, age=age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select id, date_of_birth  from user'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only select date of birth from user table\n",
    "sql = \"select id, date_of_birth \" \\\n",
    "        \" from user\"\n",
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_dob = spark_session.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|      date_of_birth|\n",
      "+---+-------------------+\n",
      "|  1|1961-02-02 00:00:00|\n",
      "|  2|1962-03-03 00:00:00|\n",
      "|  3|1963-04-04 00:00:00|\n",
      "|  4|1964-05-05 00:00:00|\n",
      "|  5|1965-06-06 00:00:00|\n",
      "|  6|1966-07-07 00:00:00|\n",
      "|  7|1967-08-08 00:00:00|\n",
      "|  8|1968-09-09 00:00:00|\n",
      "|  9|1969-10-10 00:00:00|\n",
      "| 10|1970-11-11 00:00:00|\n",
      "| 11|1971-12-12 00:00:00|\n",
      "| 12|1972-01-13 00:00:00|\n",
      "| 13|1973-02-14 00:00:00|\n",
      "| 14|1974-03-15 00:00:00|\n",
      "| 15|1975-04-16 00:00:00|\n",
      "| 16|1976-05-17 00:00:00|\n",
      "| 17|1977-06-18 00:00:00|\n",
      "| 18|1978-07-19 00:00:00|\n",
      "| 19|1979-08-20 00:00:00|\n",
      "| 20|1960-09-21 00:00:00|\n",
      "+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_dob.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function calculate_age_from_dob for each row of dataframe\n",
    "df_user_age = df_user_dob.rdd.map(calculate_age_from_dob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_age = spark_session.createDataFrame(df_user_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1| 58|\n",
      "|  2| 57|\n",
      "|  3| 56|\n",
      "|  4| 55|\n",
      "|  5| 54|\n",
      "|  6| 53|\n",
      "|  7| 52|\n",
      "|  8| 51|\n",
      "|  9| 49|\n",
      "| 10| 48|\n",
      "| 11| 47|\n",
      "| 12| 47|\n",
      "| 13| 46|\n",
      "| 14| 45|\n",
      "| 15| 44|\n",
      "| 16| 43|\n",
      "| 17| 42|\n",
      "| 18| 41|\n",
      "| 19| 40|\n",
      "| 20| 59|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show calculated age dataframe\n",
    "df_user_age.select('id', 'age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use python function to scan the whole table for each row\n",
    "**De-duplicate the users by matching the first name and the last name. For each row in user table, generate a field called superior_id by find the user with the latest modified_date among the matched users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pyspark dataframe to pandas dataframe, which will be scanned for each user\n",
    "pdf_user = df_user.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the superior_id\n",
    "def find_superior_user(row):\n",
    "    first_name, last_name = row.first_name, row.last_name\n",
    "    pdf_matched_users = pdf_user[(pdf_user['first_name'] == first_name) & (pdf_user['last_name'] == last_name)]\n",
    "    pdf_matched_users = pdf_matched_users.sort_values(by=['modified_date'], ascending=False)\n",
    "    return Row(id=row.id, superior_id=int(pdf_matched_users.iloc[0,:]['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function of finding superior user to the user dataframe\n",
    "df_superior_user = df_user.rdd.map(find_superior_user)\n",
    "df_superior_user = spark_session.createDataFrame(df_superior_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|superior_id|\n",
      "+---+-----------+\n",
      "|  1|         21|\n",
      "|  2|         22|\n",
      "|  3|         23|\n",
      "|  4|         24|\n",
      "|  5|         25|\n",
      "|  6|         26|\n",
      "|  7|         27|\n",
      "|  8|         28|\n",
      "|  9|         29|\n",
      "| 10|         20|\n",
      "| 11|         21|\n",
      "| 12|         22|\n",
      "| 13|         23|\n",
      "| 14|         24|\n",
      "| 15|         25|\n",
      "| 16|         26|\n",
      "| 17|         27|\n",
      "| 18|         28|\n",
      "| 19|         29|\n",
      "| 20|         20|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_superior_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register superior user dataframe as a table\n",
    "register_dataframe_as_table(df_superior_user, 'superior_user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select u.id, u.first_name, u.last_name, u.modified_date, su.superior_id,  sud.first_name s_first_name, sud.last_name s_last_name, sud.modified_date s_modified_date from superior_user su  left join user u on su.id = u.id  left join user sud on sud.id = su.superior_id '"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show user details and the details of the superior user\n",
    "sql = \"select u.id, u.first_name, u.last_name, u.modified_date, su.superior_id, \" \\\n",
    "        \" sud.first_name s_first_name, sud.last_name s_last_name, sud.modified_date s_modified_date\" \\\n",
    "        \" from superior_user su \" \\\n",
    "        \" left join user u on su.id = u.id \" \\\n",
    "        \" left join user sud on sud.id = su.superior_id \"\n",
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_superior_user_details = spark_session.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+-------------------+-----------+------------+-----------+-------------------+\n",
      "| id|first_name|last_name|      modified_date|superior_id|s_first_name|s_last_name|    s_modified_date|\n",
      "+---+----------+---------+-------------------+-----------+------------+-----------+-------------------+\n",
      "|  1|   fname_2|  lname_2|2019-09-02 00:00:00|         21|     fname_2|    lname_2|2019-09-22 00:00:00|\n",
      "|  2|   fname_3|  lname_3|2019-09-03 00:00:00|         22|     fname_3|    lname_3|2019-09-23 00:00:00|\n",
      "|  3|   fname_4|  lname_4|2019-09-04 00:00:00|         23|     fname_4|    lname_4|2019-09-24 00:00:00|\n",
      "|  4|   fname_5|  lname_5|2019-09-05 00:00:00|         24|     fname_5|    lname_5|2019-09-25 00:00:00|\n",
      "|  5|   fname_6|  lname_6|2019-09-06 00:00:00|         25|     fname_6|    lname_6|2019-09-26 00:00:00|\n",
      "|  6|   fname_7|  lname_7|2019-09-07 00:00:00|         26|     fname_7|    lname_7|2019-09-27 00:00:00|\n",
      "|  7|   fname_8|  lname_8|2019-09-08 00:00:00|         27|     fname_8|    lname_8|2019-09-28 00:00:00|\n",
      "|  8|   fname_9|  lname_9|2019-09-09 00:00:00|         28|     fname_9|    lname_9|2019-09-29 00:00:00|\n",
      "|  9|  fname_10| lname_10|2019-09-10 00:00:00|         29|    fname_10|   lname_10|2019-09-30 00:00:00|\n",
      "| 10|   fname_1|  lname_1|2019-09-11 00:00:00|         20|     fname_1|    lname_1|2019-09-21 00:00:00|\n",
      "| 11|   fname_2|  lname_2|2019-09-12 00:00:00|         21|     fname_2|    lname_2|2019-09-22 00:00:00|\n",
      "| 12|   fname_3|  lname_3|2019-09-13 00:00:00|         22|     fname_3|    lname_3|2019-09-23 00:00:00|\n",
      "| 13|   fname_4|  lname_4|2019-09-14 00:00:00|         23|     fname_4|    lname_4|2019-09-24 00:00:00|\n",
      "| 14|   fname_5|  lname_5|2019-09-15 00:00:00|         24|     fname_5|    lname_5|2019-09-25 00:00:00|\n",
      "| 15|   fname_6|  lname_6|2019-09-16 00:00:00|         25|     fname_6|    lname_6|2019-09-26 00:00:00|\n",
      "| 16|   fname_7|  lname_7|2019-09-17 00:00:00|         26|     fname_7|    lname_7|2019-09-27 00:00:00|\n",
      "| 17|   fname_8|  lname_8|2019-09-18 00:00:00|         27|     fname_8|    lname_8|2019-09-28 00:00:00|\n",
      "| 18|   fname_9|  lname_9|2019-09-19 00:00:00|         28|     fname_9|    lname_9|2019-09-29 00:00:00|\n",
      "| 19|  fname_10| lname_10|2019-09-20 00:00:00|         29|    fname_10|   lname_10|2019-09-30 00:00:00|\n",
      "| 20|   fname_1|  lname_1|2019-09-21 00:00:00|         20|     fname_1|    lname_1|2019-09-21 00:00:00|\n",
      "+---+----------+---------+-------------------+-----------+------------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_superior_user_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
